{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn core\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, GridSearchCV)\n",
    "from sklearn.preprocessing import (OneHotEncoder, StandardScaler, FunctionTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay)\n",
    "from scipy import sparse\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from joblib import Memory\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"project_adult.csv\")\n",
    "validation_df = pd.read_csv(\"project_validation_inputs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform basic data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME THE FIRST COLUMN (THIS IS THE INDEX COLUMN)\n",
    "\n",
    "train_df = train_df.rename(columns={\"Unnamed: 0\": \"index\"})\n",
    "validation_df = validation_df.rename(columns={\"Unnamed: 0\": \"index\"})\n",
    "\n",
    "# CLEAN THE DATA:\n",
    "def basic_clean(df):\n",
    "    # standardize column names\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "            .str.strip() # remove leading/trailing spaces\n",
    "            .str.lower() # convert to lowercase\n",
    "            .str.replace(\" \", \"_\") # replace spaces with underscore\n",
    "            .str.replace(\"-\", \"_\") # replace hyphens with underscore\n",
    "    )\n",
    "    df = df.replace(\"?\", np.nan) # replace \"?\" with NaN within data\n",
    "\n",
    "    # clean data within columns\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df[col] = df[col].str.lower()# convert to lowercase\n",
    "        df[col] = df[col].str.strip() # remove leading/trailing spaces\n",
    "        df[col] = df[col].str.replace(\"-\", \"_\") # replace hyphens with underscore\n",
    "    return df\n",
    "\n",
    "# APPLY THE CLEANING FUNCTION:\n",
    "train_df = basic_clean(train_df)\n",
    "validation_df = basic_clean(validation_df)\n",
    "\n",
    "# normalize income labels:\n",
    "train_df[\"income\"] = train_df[\"income\"].str.strip()\n",
    "\n",
    "# adjust income values to be binary\n",
    "    # <=50K -> 0  (negative class)\n",
    "    #  >50K -> 1  (positive class)\n",
    "train_df[\"income\"] = train_df[\"income\"].replace({\n",
    "    \"<=50k\": 0, \"<=50k.\": 0,\n",
    "    \">50k\": 1,  \">50k.\": 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results\n",
    "print(train_df[\"income\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", validation_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of NAs in training data\n",
    "print(\"\\nMissing count per variable in training data:\")\n",
    "print(train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of NAs in validation data\n",
    "print(\"\\nMissing count per variable in validation data:\")\n",
    "print(validation_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count each class and normalize to percentage\n",
    "income_counts = train_df[\"income\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(income_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate % earning >50K per hours per week\n",
    "hours_income = (\n",
    "    train_df.groupby([\"hours_per_week\"])[\"income\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "# convert proportions to percentages\n",
    "hours_income[\"income\"] = hours_income[\"income\"] * 100\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(hours_income[\"hours_per_week\"], hours_income[\"income\"], color=\"skyblue\")\n",
    "\n",
    "plt.xlabel(\"% earning >50K\")\n",
    "plt.ylabel(\"Hrs Worked Per Week\")\n",
    "plt.title(\"Percentage of Individuals Earning >50K by Hrs Worked Per Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate % earning >50K per education level\n",
    "edu_income = (\n",
    "    train_df.groupby([\"education\", \"education_num\"])[\"income\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .sort_values(\"education_num\")  # sort by numeric education level\n",
    ")\n",
    "\n",
    "# convert proportions to percentages\n",
    "edu_income[\"income\"] = edu_income[\"income\"] * 100\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(edu_income[\"education\"], edu_income[\"income\"], color=\"skyblue\")\n",
    "\n",
    "plt.xlabel(\"% earning >50K\")\n",
    "plt.ylabel(\"Education Level\")\n",
    "plt.title(\"Percentage of Individuals Earning >50K by Education Level (Sorted by Education Num)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore prevalance of age groups:\n",
    "\n",
    "# group ages by decade \n",
    "age_counts_by_decade = (\n",
    "    train_df.groupby((train_df[\"age\"] // 10) * 10)  # integer division → 10s, 20s, 30s, etc.\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "            .rename(columns={\"age\": \"age_decade\"})  # rename column for clarity\n",
    "            .sort_values(\"age_decade\")\n",
    ")\n",
    "\n",
    "print(age_counts_by_decade.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show each level of education, the number associated with it, and count of each for better understanding\n",
    "edu_counts = (\n",
    "    train_df.groupby([\"education_num\", \"education\"])\n",
    "            .size()                               # counts\n",
    "            .reset_index(name=\"count\")            # put into a column\n",
    "            .sort_values(\"education_num\")         # orders by the numeric code\n",
    ")\n",
    "\n",
    "#print\n",
    "print(edu_counts.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding categorical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target from features (TRAIN ONLY)\n",
    "y_train = train_df[\"income\"] # target\n",
    "X_train = train_df.drop(columns=[\"income\"])\n",
    "\n",
    "# stratified split \n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size = 0.20, \n",
    "    stratify = y_train, \n",
    "    random_state = 42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFY COLUMN TYPES: \n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "# print\n",
    "print(\"Categorical columns:\", categorical_cols.tolist())\n",
    "print(\"Numerical columns:\", numerical_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TRANSFORMERS (processing steps):\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # fill NaN with most common\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")), # fill NaN with median\n",
    "    (\"scaler\", StandardScaler()) # standardize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE INTO PREPROCESSER\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, numerical_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT ON TRAINING, TRANSFORM BOTH\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_validation_processed = preprocessor.transform(validation_df)\n",
    "\n",
    "print(\"Processed training shape:\", X_train_processed.shape)\n",
    "print(\"Processed validation shape:\", X_validation_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK FOR NAs:\n",
    "def has_nan_or_inf(X):\n",
    "    data = X.data if sparse.issparse(X) else X\n",
    "    return np.isnan(data).any(), np.isinf(data).any()\n",
    "\n",
    "print(\"Train processed:  NA/Inf:\", has_nan_or_inf(X_train_processed))\n",
    "print(\"Validation processed:  NA/Inf:\", has_nan_or_inf(X_validation_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP MODEL:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache to speed up repeated preprocessing\n",
    "memory = Memory(location=os.path.join(os.getcwd(), \"skcache\"), verbose=0)\n",
    "\n",
    "# upgraded MLP: matches baseline's training endurance\n",
    "tuned_mlp = MLPClassifier(\n",
    "    max_iter = 200,           # give it room to converge fully\n",
    "    early_stopping = True,    # same logic, but now consistent patience\n",
    "    n_iter_no_change = 10,    # same as baseline (no early quit)\n",
    "    tol = 1e-4,               # stricter tolerance, don't bail too early\n",
    "    batch_size = 256,         # match baseline batch size\n",
    "    solver = \"adam\",\n",
    "    random_state = 42         # same seed for fair comparison\n",
    ")\n",
    "\n",
    "# pipeline: preprocessing + MLP\n",
    "tuned_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"mlp\", tuned_mlp)\n",
    "], memory=memory)\n",
    "\n",
    "# focused but powerful grid\n",
    "tuned_param_grid = {\n",
    "    \"mlp__hidden_layer_sizes\": [(64,), (128,), (128, 64), (96, 48)],\n",
    "    \"mlp__activation\": [\"relu\", \"tanh\"],\n",
    "    \"mlp__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "    \"mlp__learning_rate_init\": [5e-4, 1e-3]\n",
    "    # optional: if you want to let it search endurance knobs too:\n",
    "    # \"mlp__tol\": [1e-4, 5e-4],\n",
    "    # \"mlp__n_iter_no_change\": [10, 15],\n",
    "    # \"mlp__batch_size\": [128, 256],\n",
    "}\n",
    "\n",
    "# stronger, more stable CV than the old 3-fold\n",
    "cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# GridSearchCV with multiple metrics, refit on F1\n",
    "grid = GridSearchCV(\n",
    "    estimator = tuned_pipe,\n",
    "    param_grid = [tuned_param_grid],\n",
    "    scoring = {\n",
    "        \"acc\": \"accuracy\",\n",
    "        \"f1\": \"f1\",\n",
    "        \"roc\": \"roc_auc\"\n",
    "    },\n",
    "    refit = \"f1\",\n",
    "    cv = cv5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model and print insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 table\n",
    "cvres = pd.DataFrame(grid.cv_results_).sort_values(\"rank_test_f1\")\n",
    "cols = [\"param_mlp__hidden_layer_sizes\",\"param_mlp__activation\",\"param_mlp__alpha\",\n",
    "        \"param_mlp__learning_rate_init\",\"mean_test_acc\",\"mean_test_f1\",\"mean_test_roc\",\"rank_test_f1\"]\n",
    "for c in cols:\n",
    "    if c not in cvres: cvres[c] = np.nan\n",
    "display(cvres[cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit on all labeled data, get best pipeline\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y_pred = best_model.predict(X_te)\n",
    "\n",
    "# ERROR CHECK:\n",
    "   # if predictions are strings, map to 0 or 1:\n",
    "y_true_bin = pd.Series(y_te).astype(int).values # y_true is already 0/1\n",
    "if pd.Series(y_pred).dtype == \"O\":\n",
    "    y_pred_bin = (pd.Series(y_pred) == \">50K\").astype(int).values\n",
    "else:                                        \n",
    "    y_pred_bin = pd.Series(y_pred).astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted probability for AUC calculation\n",
    "probs = best_model.predict_proba(X_te)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print report\n",
    "print(\"Holdout Accuracy:\", accuracy_score(y_true_bin, y_pred_bin))\n",
    "print(\"Holdout F1:\", f1_score(y_true_bin, y_pred_bin))\n",
    "print(\"Holdout ROC AUC:\", roc_auc_score(y_true_bin, probs))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true_bin, y_pred_bin))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true_bin, y_pred_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VISUALIZATIONS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract loss curve from the trained model\n",
    "loss_curve = best_model.named_steps[\"mlp\"].loss_curve_\n",
    "\n",
    "# plot training loss per epoch\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(loss_curve)+1), loss_curve, marker='o', color='purple')\n",
    "plt.title(\"MLP Training Loss per Epoch\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Training Loss Curve:\n",
    "- y-axis shows training loss (model's error)\n",
    "- x-axis shows epochs (iterations)\n",
    "\n",
    "Results:\n",
    "- loss decreases during first few epochs (model quickly picks up on patterns)\n",
    "- gradual flattening after 10 epochs - training stabilizes, suggests convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix\n",
    "cm = confusion_matrix(y_te, best_model.predict(X_te))\n",
    "\n",
    "# create and plot display\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"<=50K\", \">50K\"])\n",
    "disp.plot(cmap=\"Purples\", values_format=\"d\")\n",
    "\n",
    "# formatting\n",
    "plt.title(\"Confusion Matrix – Tuned MLP\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix:\n",
    "- shows number of correct/misclassified predictions\n",
    "\n",
    "Results:\n",
    "- model achieves approximately 85% accuracy\n",
    "- correctly predicts most =<50K cases\n",
    "- correctly predicts about 56-61% of >50K cases\n",
    "- false negative rate: approx. 44% (meaning around 4 in 10 individuals in >50K class were misclassified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(best_model, X_te, y_te)\n",
    "plt.title(\"ROC Curve – Tuned MLP\")\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve:\n",
    "- shows trade-off between true/false positive rates across thresholds\n",
    "- y-axis shows how well the model correctly ID >50K earners\n",
    "- x-axis shows how often the model incorrectly labels <=50K earners as >50K\n",
    "\n",
    "Results:\n",
    "- AUC = 0.91, so theres a 91% probability the model correctly gives >50K person a higher score (prob of >50K) than the <=50K person\n",
    "- model effectively distinguishes between the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(best_model, X_te, y_te, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "fi_raw = pd.Series(result.importances_mean, index=X_te.columns).sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "fi_raw.tail(15).plot(kind=\"barh\")\n",
    "plt.title(\"Feature Importance – Best MLP\")\n",
    "plt.xlabel(\"Importance (Mean Decrease in Accuracy)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance:\n",
    "- shows what features within the dataset influence predictions most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true class distribution\n",
    "true_counts = y_te.value_counts().rename({0: \"<=50K\", 1: \">50K\"})\n",
    "\n",
    "# predicted class distribution\n",
    "pred_counts = pd.Series(best_model.predict(X_te)).value_counts().rename({0: \"<=50K\", 1: \">50K\"})\n",
    "\n",
    "# combine into one df\n",
    "compare_df = pd.DataFrame({\n",
    "    \"True\": true_counts,\n",
    "    \"Predicted\": pred_counts\n",
    "}).T  # transpose so rows = category type, columns = income classes\n",
    "\n",
    "# plot\n",
    "compare_df.plot(\n",
    "    kind=\"bar\",\n",
    "    color=[\"skyblue\", \"salmon\"],\n",
    "    figsize=(7,5),\n",
    "    width=0.8\n",
    ")\n",
    "plt.title(\"True vs Predicted Income Distribution\")\n",
    "plt.ylabel(\"Number of Individuals\")\n",
    "plt.xlabel(\"Distribution Type\")\n",
    "plt.legend(title=\"Income Category\")\n",
    "plt.grid(axis=\"y\", linestyle=\":\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted Income Distribution: \n",
    "Results:\n",
    "- class imbalance closely resembles the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OTHER MLP ATTEMPT:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual single MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base MLP model defined\n",
    "base_mlp = MLPClassifier(\n",
    "    max_iter = 150,\n",
    "    early_stopping = True,      # stop training if validation score doesn't improve\n",
    "    n_iter_no_change = 10,                # num epochs w no improvement before stopping\n",
    "    batch_size = 256,           # num training samples used per gradient before weight update\n",
    "    learning_rate_init = 0.001, # initial learning rate for wight updates\n",
    "    random_state = 42           # random seed for reproducability\n",
    ")\n",
    "\n",
    "base_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"mlp\", base_mlp)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit/run model\n",
    "base_pipe.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions from base model\n",
    "yhat  = base_pipe.predict(X_te) # get final predicted class labels\n",
    "proba = base_pipe.predict_proba(X_te)[:, 1] # get models confidence for class 1\n",
    "\n",
    "# print report\n",
    "print(\"Holdout accuracy:\", accuracy_score(y_te, yhat))\n",
    "print(\"Holdout F1:\",      f1_score(y_te, yhat))\n",
    "print(\"Holdout ROC AUC:\", roc_auc_score(y_te, proba))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_te, yhat))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_te, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull tuned pipeline (already fitted by GridSearchCV on the training folds)\n",
    "tuned_pipe = best_model         # from: best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPARING BOTH MODELS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions & metrics\n",
    "y_pred_base  = base_pipe.predict(X_te)\n",
    "probs_base   = base_pipe.predict_proba(X_te)[:, 1]\n",
    "\n",
    "y_pred_tuned = tuned_pipe.predict(X_te)\n",
    "probs_tuned  = tuned_pipe.predict_proba(X_te)[:, 1]\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\":    [\"Baseline MLP\", \"Tuned MLP (GridSearchCV)\"],\n",
    "    \"Accuracy\": [accuracy_score(y_te, y_pred_base),\n",
    "                 accuracy_score(y_te, y_pred_tuned)],\n",
    "    \"F1 Score\": [f1_score(y_te, y_pred_base),\n",
    "                 f1_score(y_te, y_pred_tuned)],\n",
    "    \"ROC AUC\":  [roc_auc_score(y_te, probs_base),\n",
    "                 roc_auc_score(y_te, probs_tuned)]\n",
    "})\n",
    "\n",
    "# plot\n",
    "df_melted = metrics_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=df_melted, x=\"Metric\", y=\"Score\", hue=\"Model\",\n",
    "            palette=[\"#b3cde3\", \"#8856a7\"])\n",
    "\n",
    "for container in plt.gca().containers:\n",
    "    plt.bar_label(container, fmt=\"%.3f\", label_type=\"edge\", fontsize=10, padding=3)\n",
    "\n",
    "plt.title(\"Baseline vs Tuned MLP Performance\", fontsize=14)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.6, 0.95)\n",
    "plt.grid(axis=\"y\", linestyle=\":\", alpha=0.6)\n",
    "plt.legend(title=\"Model\", loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "- measures overall proportion of correct predictions across all classes\n",
    "\n",
    "Results:\n",
    "- both models are highly accurate (approx. 85%)\n",
    "- baseline model is identical to tuned model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score:\n",
    "- shows model performance based on a balance of precision (how accurately >50K cases are predicted) and recall (how many cases of >50K are correctly identified)\n",
    "\n",
    "Results:\n",
    "- tuned MLP has a slightly lower F1 score\n",
    "- indicates that GridSearch tuning didn't improve the baseline model's ability to correctly ID higher income individuals without increasing false positives. With more extensive training and tuning, this would likely change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC:\n",
    "- measures how well the model distinguishes between the two classes\n",
    "\n",
    "Results:\n",
    "- both are borderline identical, so both models distinguish classes well\n",
    "- baseline MLP performed .001% better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSIS:**\n",
    "\n",
    "Single MLP:\n",
    "- trains one NN with fixed settings\n",
    "- architecture/paramters are manually chosen (hidden layers, activation, etc.)\n",
    "- only trains once (runs the risk of over or underfitting)\n",
    "- results are based on one training run\n",
    "- performance depends on configuration\n",
    "\n",
    "Tuned MLP with GridSearch:\n",
    "- explores multiple model configs to find best performer\n",
    "- automatically tests many combos of hyperparameters\n",
    "- trains multiple times using cross validation\n",
    "- best model is selected based on metrics and averaged over folds\n",
    "- performance is more reliale and generalized bc it's tuned and validated\n",
    "\n",
    "Comparison:\n",
    "- accuracy for both was similar (around 85%)\n",
    "- tuned MLP slightly improved balance between precision and recall shown through F1 score (0.662 vs 0.652) (identifies high income group more effectively without heavily increasing false positives)\n",
    "- tuned MLP slightly improved ROC AUC, suggesting better class separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT FILE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copy of best pipeline\n",
    "best_full = clone(grid.best_estimator_)\n",
    "\n",
    "# Refit best model on ALL labeled training data\n",
    "best_full.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for unlabeled validation inputs\n",
    "val_pred = best_full.predict(validation_df)\n",
    "\n",
    "# Convert predictions to required format: 1 if >50K, else -1\n",
    "# First ensure predictions are binary (0 or 1)\n",
    "if pd.Series(val_pred).dtype == \"O\":\n",
    "    # If predictions are strings, convert to binary\n",
    "    val_pred_bin = (pd.Series(val_pred).str.strip().isin([\">50K\", \">50K.\", \">50k\", \">50k.\"])).astype(int)\n",
    "else:\n",
    "    val_pred_bin = pd.Series(val_pred).astype(int)\n",
    "\n",
    "# Transform: 1 stays as 1 (>50K), 0 becomes -1 (<=50K)\n",
    "val_pred_transformed = val_pred_bin.replace({0: -1, 1: 1})\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Value counts of transformed predictions:\")\n",
    "print(val_pred_transformed.value_counts())\n",
    "print(f\"\\nTotal predictions: {len(val_pred_transformed)}\")\n",
    "\n",
    "# Save to required CSV file\n",
    "output_df = pd.DataFrame({\"income\": val_pred_transformed})\n",
    "output_df.to_csv(\"Group_11_MLP_PredictedOutputs.csv\", index=False)\n",
    "\n",
    "print(\"\\n✓ Predictions saved to 'Group_11_MLP_PredictedOutputs.csv'\")\n",
    "print(f\"  Format: 1 for >50K, -1 for <=50K\")\n",
    "\n",
    "# Preview first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(output_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
